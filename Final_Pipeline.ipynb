{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "G_VWoI1Oo_jZ"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "      try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration()])\n",
    "      except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "KHOvCxl4pITV"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "wEY3kIOzo_jZ"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O8P0hAsnsULB",
    "outputId": "4c5f7a27-1920-4241-c8ac-7dee27e26740"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/pratiksha/anaconda3/lib/python3.8/site-packages (3.6.5)\r\n",
      "Requirement already satisfied: click in /home/pratiksha/anaconda3/lib/python3.8/site-packages (from nltk) (8.0.3)\r\n",
      "Requirement already satisfied: joblib in /home/pratiksha/anaconda3/lib/python3.8/site-packages (from nltk) (1.1.0)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /home/pratiksha/anaconda3/lib/python3.8/site-packages (from nltk) (2021.8.3)\r\n",
      "Requirement already satisfied: tqdm in /home/pratiksha/anaconda3/lib/python3.8/site-packages (from nltk) (4.62.3)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Hjb20Azvik_u"
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kx3E06Klscez",
    "outputId": "7b00d87e-e371-43dc-fc33-96e5f2641a0e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/pratiksha/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "JA9hHnk-o_jq"
   },
   "outputs": [],
   "source": [
    "def Get_Article_wiki(file_name):\n",
    "    fpath = 'text_data/'+file_name+'.txt.clean'\n",
    "    f=open(fpath,'r',encoding='ISO-8859-1')\n",
    "    text = f.read()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "i93_SFTto_jr"
   },
   "outputs": [],
   "source": [
    "def Text_Preprocess(text):\n",
    "  text = str(text)\n",
    "  text = text.lower()\n",
    "  words = re.sub(r'[^\\w\\s]', '', text).split()\n",
    "  return \" \".join([word for word in words])\n",
    "def cleanText(text):\n",
    "  text = str(text)\n",
    "  text = text.lower()\n",
    "  words = re.sub(r'[^\\w\\s\\.\\?]', '', text).split()\n",
    "  return \" \".join([word for word in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "39QvSiPwoiyT",
    "outputId": "94cfc77c-b3a3-49bd-c766-ea566bd436f3"
   },
   "outputs": [],
   "source": [
    "def decontractions(phrase):\n",
    "    \"\"\"decontracted takes text and convert contractions into natural form.\n",
    "     ref: https://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python/47091490#47091490\"\"\"\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"won\\’t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\’t\", \"can not\", phrase)\n",
    "\n",
    "    # general\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "\n",
    "    phrase = re.sub(r\"n\\’t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\’re\", \" are\", phrase)\n",
    "    phrase = re.sub(r\"\\’s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\’d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\’ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\’t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\’ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"\\’m\", \" am\", phrase)\n",
    "\n",
    "    return phrase\n",
    "\n",
    "def preprocess(text):\n",
    "    # convert all the text into lower letters\n",
    "    # use this function to remove the contractions: https://gist.github.com/anandborad/d410a49a493b56dace4f814ab5325bbd\n",
    "    # remove all the spacial characters: except space ' '\n",
    "    text = text.lower()\n",
    "    text = decontractions(text)\n",
    "    text = re.sub('[^A-Za-z0-9 ]+', '', text)\n",
    "    return text\n",
    "\n",
    "# def preprocess_ita(text):\n",
    "#     # convert all the text into lower letters\n",
    "#     # remove the words betweent brakets ()\n",
    "#     # remove these characters: {'$', ')', '?', '\"', '’', '.',  '°', '!', ';', '/', \"'\", '€', '%', ':', ',', '('}\n",
    "#     # replace these spl characters with space: '\\u200b', '\\xa0', '-', '/'\n",
    "#     # we have found these characters after observing the data points, feel free to explore more and see if you can do find more\n",
    "#     # you are free to do more proprocessing\n",
    "#     # note that the model will learn better with better preprocessed data \n",
    "    \n",
    "#     text = text.lower()\n",
    "#     text = decontractions(text)\n",
    "#     text = re.sub('[$)\\?\"’.°!;\\'€%:,(/]', '', text)\n",
    "#     text = re.sub('\\u200b', ' ', text)\n",
    "#     text = re.sub('\\xa0', ' ', text)\n",
    "#     text = re.sub('-', ' ', text)\n",
    "#     return text\n",
    "\n",
    "\n",
    "# all_data['Question'] = all_data['Question'].apply(preprocess)\n",
    "# all_data['Answer'] = all_data['Answer'].apply(preprocess)\n",
    "# all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "eikgI--6o_js"
   },
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn import preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "O0oqEj76o_js"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense,Input,Activation\n",
    "from tensorflow.keras.models import Model\n",
    "import random as rn\n",
    "from sklearn import metrics\n",
    "import keras.backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "# from tensorflow.keras.layers import Dense,Input\n",
    "from tensorflow.keras.layers import MaxPooling1D\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Embedding\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import LSTM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "LQDULkP0o_j0"
   },
   "outputs": [],
   "source": [
    "def predict(input_sentence,model_dot):\n",
    "\n",
    "  '''\n",
    "  A. Given input sentence, convert the sentence into integers using tokenizer used earlier\n",
    "  B. Pass the input_sequence to encoder. we get encoder_outputs, last time step hidden and cell state\n",
    "  C. Initialize index of <start> as input to decoder. and encoder final states as input_states to onestepdecoder.\n",
    "  D. till we reach max_length of decoder or till the model predicted word <end>:\n",
    "         predictions, input_states, attention_weights = model.layers[1].onestepdecoder(input_to_decoder, encoder_output, input_states)\n",
    "         Save the attention weights\n",
    "         And get the word using the tokenizer(word index) and then store it in a string.\n",
    "  E. Call plot_attention(#params)\n",
    "  F. Return the predicted sentence\n",
    "  '''\n",
    "  inp_seq = tknizer_que.texts_to_sequences([input_sentence])\n",
    "  inp_seq = pad_sequences(inp_seq,padding='post',maxlen=30)\n",
    "  # print(inp_seq)\n",
    "  initial_state=model_dot.layers[0].initialize_states(10)  \n",
    "  # print(initial_state)\n",
    "  en_outputs,state_h,state_c = model_dot.layers[0](tf.constant(inp_seq),initial_state)\n",
    "  # print(state_h)\n",
    "  cur_vec = tf.constant([[tknizer_ans.word_index['<start>']]])\n",
    "  pred = []\n",
    "  #Here 20 is the max_length of the sequence\n",
    "  # states=[state_h,state_c]\n",
    "  attention_plot = np.zeros((30, 30))\n",
    "  for i in range(30):\n",
    "    # i=i+1\n",
    "        infe_output, state_h, state_c,attention_weights ,context_vector= model_dot.layers[1].onestepdecoder(cur_vec,en_outputs,state_h,state_c)\n",
    "        # attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        # attention_plot[i]=(attention_weights.numpy())\n",
    "        # print(attention_plot[i])\n",
    "        infe_output =  tf.expand_dims(infe_output, 1)\n",
    "#         print(infe_output)\n",
    "        cur_vec = np.reshape(np.argmax(infe_output),(1,1))\n",
    "#         print(cur_vec)\n",
    "        if (cur_vec == [[1215]] and i == 0):\n",
    "#             pred.append(tknizer_ans.index_word[cur_vec[0][0]])\n",
    "            translated_sentence = 'nan'\n",
    "        elif(cur_vec == [[0]]):\n",
    "        # print(tknizer_eng.index_word[cur_vec[0][0]])\n",
    "        #     print(pred)\n",
    "            continue\n",
    "        pred.append(tknizer_ans.index_word[cur_vec[0][0]])\n",
    "        \n",
    "        if(pred[-1]=='<end>'):\n",
    "            break\n",
    "        translated_sentence = ' '.join(pred)\n",
    "\n",
    "  return translated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from keras.models import load_model\n",
    "\n",
    "model_dot=load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below code from below website\n",
    "# https://medium.com/swlh/building-your-first-qa-chatbot-with-python-eb00bacebfb\n",
    "GREETING_INPUTS = (\"namastey\",\"namaskaram\",\"hello\", \"hi\", \"whats up\",\"hey\")\n",
    "GREETING_RESPONSES = [\"namastey\",\"namaskaram\",\"hello\", \"hi\", \"whats up\",\"hey\"]\n",
    "def greeting(sentence):\n",
    " \n",
    "    for word in sentence.split():\n",
    "        if word.lower() in GREETING_INPUTS:\n",
    "            return random.choice(GREETING_RESPONSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QaBot: Namaskaram I am QaBot. I am an expert in politics,geography etc ask me anything. Go ahead!\n",
      "hi\n",
      "QaBot: hey\n",
      "how did vespoid wasps evolve\n",
      "QaBot: there are in in million\n",
      "do butterflies make sounds\n",
      "QaBot: some butterflies make sounds\n",
      "what do we refer musicians who play flute\t\n",
      "QaBot: a flute player a flautist or a flutist\n",
      "how many verb paradigms are there in korean\t\n",
      "QaBot: there are seven verb paradigms or speech levels in korean\n",
      "are elephants mammals\t\n",
      "QaBot: yes\n",
      "are elephants good swimmers\t\n",
      "QaBot: yes\n",
      "is the kangaroo an herbivour\n",
      "QaBot: nan\n",
      "was santiago the name of an indiepunk band\n",
      "QaBot: yes\n",
      "where is there a newton statue on display\n",
      "QaBot: a person\n",
      "what is largest polar bear on record\n",
      "QaBot: 2200 lb\n",
      " what was anders celsiuss profession\n",
      "QaBot: six\n",
      "how large is the population of san francisco\n",
      "QaBot: san francisco has an estimated population of 808976\n",
      "thanks\n",
      "Qabot: Anytime\n"
     ]
    }
   ],
   "source": [
    "flag=True\n",
    "print(\"QaBot: Namaskaram I am QaBot. I am an expert in politics,geography etc ask me anything. Go ahead!\")\n",
    "while(flag==True):\n",
    "    message = input(\"\")\n",
    "    message = Text_Preprocess(message)\n",
    "    message = cleanText(message)\n",
    "    message = decontractions(message)\n",
    "    message = preprocess(message)\n",
    "    if(message!='bye'):\n",
    "        if(message=='thanks' or message=='thank you' ):\n",
    "            flag=False\n",
    "            print(\"Qabot: Anytime\")\n",
    "        else:\n",
    "            if(greeting(message)!=None):\n",
    "                print(\"QaBot: \"+greeting(message))\n",
    "            else:\n",
    "                \n",
    "                print(\"QaBot: \",end=\"\")\n",
    "                print(predict(message,model_dot))\n",
    "    else:\n",
    "        flag=False\n",
    "        print(\"QaBot: take care..\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of Model_train_QA_Bot_-Copy1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
